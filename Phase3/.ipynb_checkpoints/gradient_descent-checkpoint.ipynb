{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Solving-the-Line-of-Best-Fit-by-Guessing\" data-toc-modified-id=\"Solving-the-Line-of-Best-Fit-by-Guessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Solving the Line of Best Fit by Guessing</a></span></li><li><span><a href=\"#The-Loss-Function\" data-toc-modified-id=\"The-Loss-Function-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>The Loss Function</a></span></li><li><span><a href=\"#The-Cost-Function\" data-toc-modified-id=\"The-Cost-Function-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>The Cost Function</a></span></li><li><span><a href=\"#Better-Way-of-Guessing:-Gradient-Descent\" data-toc-modified-id=\"Better-Way-of-Guessing:-Gradient-Descent-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Better Way of Guessing: Gradient Descent</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gradient-Descent-in-Words\" data-toc-modified-id=\"Gradient-Descent-in-Words-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Gradient Descent in Words</a></span></li><li><span><a href=\"#Stepping-Down-a-Hill:-Step-Size\" data-toc-modified-id=\"Stepping-Down-a-Hill:-Step-Size-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Stepping Down a Hill: Step Size</a></span></li><li><span><a href=\"#Putting-It-All-Together\" data-toc-modified-id=\"Putting-It-All-Together-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Putting It All Together</a></span></li></ul></li><li><span><a href=\"#Level-Up:-Gradient-Descent-Walk-Through\" data-toc-modified-id=\"Level-Up:-Gradient-Descent-Walk-Through-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Level Up: Gradient Descent Walk Through</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Explain and use the concept of a gradient\n",
    "- Explain the algorithm of gradient descent\n",
    "- Describe the effect of the \"learning rate\" in the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Solving the Line of Best Fit by Guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's say we have some data below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Randomly created data in x & y\n",
    "np.random.seed(27)\n",
    "\n",
    "x = np.random.rand(30, 1).reshape(30)\n",
    "y_randterm = np.random.normal(0, 3, 30)\n",
    "y = 3 + 50 * x + y_randterm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here's the data plotted out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGoCAYAAABL+58oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAf/klEQVR4nO3df5RkZX3n8ffXoTUtYBp0YGdaEDWkwQ3CmPZHwib+IMn4g8gs628jo8uGY7LxuGfdiTNu1jWb3TDZOcc1e1bXcIw6xp+sTgbUxJGFoCYiZLDF0WAvhCDSM8IoNiC2Oozf/aNuQ0/bP6p6uu6teur9OqdOVd26t+rbfU9PfeZ5nvs8kZlIkiSV5BFNFyBJkrTaDDiSJKk4BhxJklQcA44kSSqOAUeSJBXHgCNJkopjwJFUq4j4fkQ8qek6mhARr42Iv21z3/dHxH/tdk1SqQw4Uh+KiNsjYiYi7o+I6Yj4YkS8PiLa+puOiNMiIiPimKOoISPigSqwTEXE2yNizXLHZeZxmXlbB5/xc0dR30qPnf39fHne9sdFxI8j4vaVvK+k+hhwpP71m5l5PPAEYDvwZuDPa67h7Mw8DjgPeBXw2zV/frcdGxG/MOf5q4B/aqoYSe0z4Eh9LjPvzcwrgZcDm2e/kCPiRRExERH3RcS3IuJtcw77fHU/XbXA/FJEPDkiromI70bEdyLiQxEx0mYN3wC+AMx+9m9HxK0RcU9EXBkR62f3nduyUnXDvDMiPl21Rl0fEU+uXput8aaqxpdXLSifqlqt7omILyzUarXQscvVtYi/ADbPeX4R8IF5n3VmRFxb1fT1iHjxnNceW33OfRFxA/DkeceeERFXVfVMRsTLlqlHUpsMOFIhMvMG4E7gV6pND9D6Qh4BXgT8TkRsql771ep+pOoyug4I4FJgPXAmcArwtnY+OyKeUn3uREQ8r3qflwHrgG8CH13i8FcCfwicANwK/Lfq55mt8eyqxo8Bb6p+xrXAycBbgJ9ab2ahY1dQF8AHgVdExJqIOBM4Hrh+zs89BHwS+CxwEvAG4EMRMVbt8k7gh9Xn/evqNnvsscBVwIerY18JvCsi/vkyNUlqgwFHKst+4ESAzLw2M/dl5k8y86vAR4BnL3ZgZt6amVdl5o8y8yDw9qX2r3w5Ir5H60v+PcD7gFcD783ML2fmj4BtwC9FxGmLvMeuzLwhMx8EPgScs8TnHaIVFp6QmYcy8wvZ/oJ6ndYFrTA1CfwarZacD8x7/VnAccD2zPxxZl4DfAp4ZTUe6V8Bb83MBzLza8DOOceeD9yeme/LzAcz88vAJ4CXtPnzSFqCAUcqyyhwD0BEPDMi/iYiDkbEvcDrgcctdmBEnBQRH60GDN9Hq/Vi0f0rT8vMEzLzyZn5B5n5E1otQN+c3SEzvw98t6ptId+e8/gHtALDYnbQauX5bETcFhFbl6lvrk7rmvUB4LW0Wlg+uMB7fqv6uWd9s3rPtcAxwLfmvTbrCcAzq66t6YiYphXC/lmbP4+kJRhwpEJExNNpfbHOXob8YeBK4JTM/Fng3bS6oWCBbh1a3TcJPDUzHwP81pz9O7Gf1pf3bF3HAo8FplbwXkfIzPsz802Z+STgN4F/HxHndbmuT9Dq4rstM78577X9wCnzxgGdWr3nQeBBWl19c1+b9S3gc5k5Mud2XGb+Tps/j6QlGHCkPhcRj4mI82mNJ/lgZu6rXjoeuCczfxgRz6B1BdCsg8BPgLnz0RwPfJ/WwONRYMsKS/ow8LqIOCciHgX8MXB9Zt6+gve6a26NEXF+RPxcRARwH3C4ui177ErryswHgOcB/2aBl6+nNdbp9yNiKCKeQyt4fTQzDwO7gLdFxKOrcUpzByx/Cvj5iHhNdexQRDy9Gusj6SgZcKT+9cmIuJ9WS8B/pDVm5nVzXv9d4L9U+7wVuHz2hcz8Aa3BvH9XdY88i9ZA36cB9wKfpvXl3LHMvBr4T7RaPg7QunLoFSt5L1qDnHdWNb4MOB34v7SC2HXAuzLz2naOPZq6MnNvZv7jAtt/DLwYeAHwHeBdwEXVVWUAv0ery+3bwPtpjVGaPfZ+4DeqGvZX+/wJ8Kh2apK0tGh/fJ4kSVJ/sAVHkiQVp9aAExEjEfHxiPhGRNxcTS52YjXR1S3V/Ql11iRJkspTdwvOnwKfycwzgLOBm4GtwNWZeTpwdfVckiRpxWobgxMRjwFuAp40d2KuiJgEnpOZByJiHXBtZo4t9j6SJEnLWfFKwivwJFqXpr4vIs4GbgTeCJycmQcAqpBz0kIHR8QlwCUAxx577C+eccYZ9VQtSZIac+ONN34nM9d2elydLTjjwJeAczPz+oj4U1rzWLwhM0fm7Pe9zFxyHM74+Hju3bu3q/VKkqTmRcSNmTne6XF1jsG5E7gzM2cXqvs4rTk37qq6pqju766xJkmSVKDaAk5mfhv41pxVds8D/oHWVPKzs3tuBq6oqyZJklSmOsfgALwB+FBEPBK4jdasq48ALo+Ii4E7gJfWXJMkSSpMrQEnM78CLNSP1u5ieZIkSctyJmNJklQcA44kSSqOAUeSJBXHgCNJkopjwJEkScUx4EiSpOIYcCRJUnEMOJIkqTgGHEmSVBwDjiRJKo4BR5IkFceAI0mSimPAkSRJxTHgSJKk4hhwJElScQw4kiSpOAYcSZJUHAOOJEkqjgFHkiQVx4AjSZKKY8CRJEnFMeBIkqTiGHAkSVJxDDiSJKk4BhxJklQcA44kSSqOAUeSJBXHgCNJkopjwJEkScUx4EiSpOIYcCRJUnEMOJIkqTgGHEmSVBwDjiRJKo4BR5IkFceAI0mSimPAkSRJxTHgSJKk4hhwJElScQw4kiSpOAYcSZJUHAOOJEkqzjFNFyBJkmD3xBQ79kyyf3qG9SPDbNk4xqYNo02X1bcMOJIkNWz3xBTbdu1j5tBhAKamZ9i2ax9AUSGnzhBnF5UkSQ3bsWfyoXAza+bQYXbsmWyootU3G+KmpmdIHg5xuyemuvJ5BhxJkhq2f3qmo+39qO4QZ8CRJKlh60eGO9rej+oOcQYcSZIatmXjGMNDa47YNjy0hi0bxxqqaPXVHeIMOJIkNWzThlEuvfAsRkeGCWB0ZJhLLzyrqAHGdYc4r6KSJKkHbNowWlSgmW/2Z6vrKioDjiRJqkWdIa7WgBMRtwP3A4eBBzNzPCJOBD4GnAbcDrwsM79XZ12SJKksTYzBeW5mnpOZ49XzrcDVmXk6cHX1XJIkacV6YZDxBcDO6vFOYFNzpUiSpBLUHXAS+GxE3BgRl1TbTs7MAwDV/UkLHRgRl0TE3ojYe/DgwZrKlSRJ/ajuQcbnZub+iDgJuCoivtHugZl5GXAZwPj4eHarQEmS1P9qbcHJzP3V/d3AXwLPAO6KiHUA1f3dddYkSZLKU1vAiYhjI+L42cfAbwBfA64ENle7bQauqKsmSZJUpjq7qE4G/jIiZj/3w5n5mYj4e+DyiLgYuAN4aY01SZKkAtUWcDLzNuDsBbZ/FzivrjokSepnuyemapsNuJ85k7EkSX1i98QU23btY+bQYQCmpmfYtmsfgCFnnl6YB0eSJLVhx57Jh8LNrJlDh9mxZ7KhinqXAUeSpD6xf3qmo+2DzIAjSVKfWD8y3NH2QWbAkSSpT2zZOMbw0JojtgWtsTjnbr+G3RNTzRTWgxxkLElSn5gdSLxjzyRT0zMErTWQwAHH89mCI0lSH9m0YZS/2/o8RkeGmb9ukQOOH2bAkSSpDzngeGkGHEmS+pADjpdmwJEkqQ8tNOB4eGgNWzaONVRRb3GQsSRJfWjugGOXbfhpBhxJkvrUpg2jBppFGHAkSaqZC2Z2nwFHkqQauWBmPRxkLElSjVwwsx4GHEmSauT8NfUw4EiSVCPnr6mHAUeSpBo5f009HGQsSSpWL16t5Pw19TDgSJKK1MtXKzl/TffZRSVJKpJXKw02W3AkSUVa6mqlXuy60uqyBUeSVKTFrkoaefQQ23btY2p6huThrqvdE1P1FqiuMuBIkoq02NVKmdh1NQAMOJKkIm3aMMqlF57F6MgwAYyODHPphWdx78yhBfd3or2yOAZHklSsha5W2rFnkqkFwowT7ZXFFhxJ0kBxor3BYAuOJGmgONHeYDDgSJIGjhPtlc8uKkmSVBwDjiRJKo5dVJKknuRswzoaBhxJUs/p5YUy1R8MOJKknrPUQpmlBxxbrlaHAUeS1HOWWiizZLZcrR4HGUuSes5iswqXPtvwUi1X6owBR5LUcwZ1tuFBbbnqBgOOJKnnLLZQZundNIPactUNjsGRJPWkQZxteMvGsSPG4MBgtFx1gwFHkqQe4TpZq8eAI0lSDxnElqtucAyOJEkqjgFHkiQVx4AjSZKKY8CRJEnFcZCxJEmLcF2o/mXAkSRpAa4L1d8MOJI0IGyN6Mwgr2heAgOOJA0AWyM657pQ/c1BxpI0AFylunOuC9XfDDiSNABsjejcoK5oXoraA05ErImIiYj4VPX8xIi4KiJuqe5PqLsmSSqdrRGdG9QVzUvRxBicNwI3A4+pnm8Frs7M7RGxtXr+5gbqkqRiuUr1yrguVP+qtQUnIh4PvAh4z5zNFwA7q8c7gU111iRJg8DWCA2aultw3gH8PnD8nG0nZ+YBgMw8EBEnLXRgRFwCXAJw6qmndrlMSSqPrREaJLW14ETE+cDdmXnjSo7PzMsyczwzx9euXbvK1UmSpJLU2YJzLvDiiHgh8DPAYyLig8BdEbGuar1ZB9xdY02SpAY5+aC6pbYWnMzclpmPz8zTgFcA12TmbwFXApur3TYDV9RVkySpObOTD05Nz5A8PPng7omppktTAXphJuPtwOURcTFwB/DShuuRJLXhaFtfXApB3dRIwMnMa4Frq8ffBc5rog5J0sqsxtIPTj6obnImY0lSx1Zj6QcnH1Q3GXAkSR1bjdYXl0JQNxlwJEkdW43WFycfVDf1wiBjSVKfWa2lH5x8UN1iwJEkdWw2lDiHjXqVAUeStCK2vqiXGXAkST3BWY21mgw4kqTGrca8OtJcXkUlSWrcasyrI81lwJEkNc5ZjbXaDDiSpMY5q7FWmwFHktQ4ZzXWanOQsSSpcc6ro9VmwJEk9QTn1dFqMuBI6lvOmyJpMQYcSX3JeVMkLcVBxpL6kvOmSFqKAUdSX3LeFElLMeBI6kvOmyJpKQYcSX3JeVMkLcVBxpL6kvOmSFqKAUdS33LeFEmLsYtKkiQVx4AjSZKKY8CRJEnFMeBIkqTiGHAkSVJxvIpKkmri4qBSfQw4klQDFweV6mUXlSTVwMVBpXoZcCSpBi4OKtXLgCNJNXBxUKleBhxJqoGLg0r1cpCxJNXAxUGlehlwJKkmLg4q1ccuKkmSVBwDjiRJKo4BR5IkFceAI0mSimPAkSRJxTHgSJKk4hhwJElScQw4kiSpOAYcSZJUHAOOJEkqjgFHkiQVx4AjSZKKY8CRJEnFMeBIkqTiGHAkSVJxDDiSJKk4tQWciPiZiLghIm6KiK9HxB9W20+MiKsi4pbq/oS6apIkSWVqO+BExO6IOD8iVhqKfgQ8LzPPBs4Bnh8RzwK2Aldn5unA1dVzSUvYPTHFuduv4YlbP825269h98RU0yVJUk/pJKw8AHwMuDMi/jgiTu/kg7Ll+9XToeqWwAXAzmr7TmBTJ+8rDZrdE1Ns27WPqekZEpianmHbrn2GHEmao+2Ak5mvBtYBfwT8GjAZEZ+PiIsiYrid94iINRHxFeBu4KrMvB44OTMPVJ9xADhpkWMviYi9EbH34MGD7ZYtFWfHnklmDh0+YtvMocPs2DPZUEWS1Hs66m7KzPsy839n5jOAs4AbgT8Dvh0RfxYRZy5z/OHMPAd4PPCMiPiFDj77sswcz8zxtWvXdlK2VJT90zMdbZekQbSi8TQRsZ5W19L5wIPAx4FTgK9GxH9Y7vjMnAauBZ4P3BUR66r3XUerdUfSItaPLNxguth2SRpEnQwyHoqIl0TEXwHfpDVW5r8D6zLz4sx8IfBq4A8WOX5tRIxUj4dpdXN9A7gS2Fztthm4YmU/ijQYtmwcY3hozRHbhofWsGXjWEMVSVLvOaaDfQ8AAXwY2JqZX11gn6uA7y1y/DpgZ0SsoRWsLs/MT0XEdcDlEXExcAfw0g5qkgbOpg2jQGsszv7pGdaPDLNl49hD2yVJEJnZ3o4RrwH+T2b+sLslLW98fDz37t3bdBmSJKnLIuLGzBzv9Li2W3Ay8y86fXNJkqQmuFSDJEkqjgFHkiQVx4AjSZKKY8CRJEnFMeBIkqTiGHAkSVJxDDiSJKk4BhxJklQcA44kSSqOAUeSJBXHgCNJkorTyWrikgq2e2LKFcolFcOAI4ndE1Ns27WPmUOHAZianmHbrn0AhhxJfckuKkns2DP5ULiZNXPoMDv2TDZUkSQdHQOOJPZPz3S0XZJ6nQFHEutHhjvaLkm9zoAjiS0bxxgeWnPEtuGhNWzZONZQRZJ0dBxkLOmhgcReRSWpFAYcSUAr5BhoJJXCLipJklQcA44kSSqOAUeSJBXHgCNJkopjwJEkScUx4EiSpOIYcCRJUnEMOJIkqTgGHEmSVBwDjiRJKo4BR5IkFceAI0mSimPAkSRJxTHgSJKk4hhwJElScQw4kiSpOAYcSZJUHAOOJEkqjgFHkiQVx4AjSZKKY8CRJEnFMeBIkqTiHNN0AZKWtntiih17Jtk/PcP6kWG2bBxj04bRpsuSpJ5mwJEW0CuhYvfEFNt27WPm0GEApqZn2LZrH4AhR5KWYBeVNM9sqJianiF5OFTsnpiqvZYdeyYfCjezZg4dZseeydprkaR+YsCR5umlULF/eqaj7ZKkFgOONE8vhYr1I8MdbZcktRhwpHl6KVRs2TjG8NCaI7YND61hy8ax2muRpH5iwJHm6aVQsWnDKJdeeBajI8MEMDoyzKUXnuUAY0lahldRSfPMhodeuIpqth4DjSR1xoAjLcBQIUn9rbYuqog4JSL+JiJujoivR8Qbq+0nRsRVEXFLdX9CXTVJkqQy1TkG50HgTZl5JvAs4N9GxFOArcDVmXk6cHX1XFIX7Z6Y4tzt1/DErZ/m3O3XNDLHjyR1U20BJzMPZOaXq8f3AzcDo8AFwM5qt53AprpqkgZRL01kKEnd0shVVBFxGrABuB44OTMPQCsEASctcswlEbE3IvYePHiwtlql5fRba0gvTWQoSd1Se8CJiOOATwD/LjPva/e4zLwsM8czc3zt2rXdK1DqQD+2hvTSRIaS1C21BpyIGKIVbj6UmbuqzXdFxLrq9XXA3XXWJB2NfmwN6aWJDCWpW+q8iiqAPwduzsy3z3npSmBz9XgzcEVdNUlHqx9bQ3ppIkNJ6pY658E5F3gNsC8ivlJtewuwHbg8Ii4G7gBeWmNN0lFZPzLM1AJhppdbQ3ptIkNJ6obaAk5m/i0Qi7x8Xl11SKtpy8Yxtu3ad0Q3VT+0hjiRoaTSOZOxdBRsDZGk3mTAkY6SrSGS1HtcTVySJBXHgCNJkopjwJEkScUx4EiSpOIYcCRJUnEMOJIkqTgGHEmSVBwDjiRJKo4BR5IkFceAI0mSimPAkSRJxTHgSJKk4hhwJElScQw4kiSpOAYcSZJUHAOOJEkqzjFNF6D+sHtiih17Jtk/PcP6kWG2bBxj04bRpsuSJGlBBhwta/fEFNt27WPm0GEApqZn2LZrH4AhR5LUk+yi0rJ27Jl8KNzMmjl0mB17JhuqSJKkpRlwtKz90zMdbZckqWl2UWlZ60eGmVogzKwfGV70GMfsSJKaZAuOlrVl4xjDQ2uO2DY8tIYtG8cW3H92zM7U9AzJw2N2dk9M1VCtJEm24GgJc1thRh49xKOOeQT3zhxatkVmqTE7tuJIkupgwNGC5l859b0fHGJ4aA3/4+XnLBtSHLMjSWqaXVRa0NFcObXY2JylxuxIkrSaDDha0NG0wnQ6ZkeSpNVmwNGCjqYVZtOGUS698CxGR4YJYHRkmEsvPMvxN5Kk2jgGRwvasnHsiDE40FkrzKYNowYaSVJjDDgDarl5amYfO5eNJKkfRWY2XUPHxsfHc+/evU2X0bfmXyEFEEDS6k4yyEiSekVE3JiZ450e5xicAbTQFVKzMddJ+SRJJTDgDKDlroRyIU1JUr8z4Aygdq6EclI+SVI/M+AMoIXmqZnPSfkkSf3Mq6hWWT+soj33Cqmp6ZmHBhjPclI+SVK/M+CsovlXJ80O2AV6MuTM1tQPoUySpE4YcFZRv66i7aR8kqTSOAZnFbmKtiRJvcGAs4pcRVuSpN5gwFlFrqItSVJvcAzOKnL9JkmSeoMBZ5U5YFeSpObZRSVJkopjwJEkScUx4EiSpOIYcCRJUnEMOJIkqTgGHEmSVBwDjiRJKk5tASci3hsRd0fE1+ZsOzEiroqIW6r7E+qqR5IklavOFpz3A8+ft20rcHVmng5cXT2XJEk6KrUFnMz8PHDPvM0XADurxzuBTXXVI0mSytX0GJyTM/MAQHV/0mI7RsQlEbE3IvYePHiwtgIlSVL/aTrgtC0zL8vM8cwcX7t2bdPlSJKkHtZ0wLkrItYBVPd3N1yPJEkqQNMB50pgc/V4M3BFg7VIkqRC1HmZ+EeA64CxiLgzIi4GtgO/HhG3AL9ePZckSToqx9T1QZn5ykVeOq+uGiRJ0mBouotKkiRp1RlwJElScQw4kiSpOAYcSZJUHAOOJEkqjgFHkiQVx4AjSZKKY8CRJEnFMeBIkqTiGHAkSVJxDDiSJKk4BhxJklQcA44kSSqOAUeSJBXHgCNJkopzTNMF9LPdE1Ps2DPJ/ukZ1o8Ms2XjGJs2jDZd1hH6oUZJklabAWeFdk9MsW3XPmYOHQZganqGbbv2AfRMgOiHGiVJ6ga7qFZox57Jh4LDrJlDh9mxZ7Khin5aP9QoSVI3GHBWaP/0TEfbm9APNUqS1A0GnBVaPzLc0fYm9EONkiR1gwFnhbZsHGN4aM0R24aH1rBl41hDFf20fqhRkqRucJDxCs0O0u3lK5T6oUZJkrohMrPpGjo2Pj6ee/fubboMSZLUZRFxY2aOd3qcXVSSJKk4BhxJklQcA44kSSqOAUeSJBXHgCNJkopjwJEkScUx4EiSpOIYcCRJUnEMOJIkqTgGHEmSVBwDjiRJKo4BR5IkFceAI0mSimPAkSRJxTHgSJKk4hhwJElScQw4kiSpOAYcSZJUHAOOJEkqjgFHkiQVx4AjSZKKY8CRJEnFMeBIkqTiGHAkSVJxDDiSJKk4BhxJklQcA44kSSqOAUeSJBWnJwJORDw/IiYj4taI2Np0PZIkqb81HnAiYg3wTuAFwFOAV0bEU5qtSpIk9bPGAw7wDODWzLwtM38MfBS4oOGaJElSHzum6QKAUeBbc57fCTxz/k4RcQlwSfX0RxHxtRpqU2ceB3yn6SJ0BM9Jb/K89CbPS28aW8lBvRBwYoFt+VMbMi8DLgOIiL2ZOd7twtQZz0vv8Zz0Js9Lb/K89KaI2LuS43qhi+pO4JQ5zx8P7G+oFkmSVIBeCDh/D5weEU+MiEcCrwCubLgmSZLUxxrvosrMByPi94A9wBrgvZn59WUOu6z7lWkFPC+9x3PSmzwvvcnz0ptWdF4i86eGu0iSJPW1XuiikiRJWlUGHEmSVJyeDjjLLeEQLf+zev2rEfG0JuocJG2ck1dX5+KrEfHFiDi7iToHTbvLnUTE0yPicES8pM76BlU75yUinhMRX4mIr0fE5+qucdC08W/Yz0bEJyPipuqcvK6JOgdNRLw3Iu5ebI67FX3fZ2ZP3mgNOP5H4EnAI4GbgKfM2+eFwF/TmkvnWcD1Tddd8q3Nc/LLwAnV4xd4TnrjvMzZ7xrgr4CXNF136bc2/15GgH8ATq2en9R03SXf2jwnbwH+pHq8FrgHeGTTtZd+A34VeBrwtUVe7/j7vpdbcNpZwuEC4APZ8iVgJCLW1V3oAFn2nGTmFzPze9XTL9Ga10jd1e5yJ28APgHcXWdxA6yd8/IqYFdm3gGQmZ6b7mrnnCRwfEQEcBytgPNgvWUOnsz8PK3f9WI6/r7v5YCz0BIOoyvYR6un09/3xbQSt7pr2fMSEaPAvwTeXWNdg66dv5efB06IiGsj4saIuKi26gZTO+fkfwFn0ppwdh/wxsz8ST3laQkdf983Pg/OEtpZwqGtZR60atr+fUfEc2kFnH/R1YoE7Z2XdwBvzszDrf+YqgbtnJdjgF8EzgOGgesi4kuZ+f+6XdyAauecbAS+AjwPeDJwVUR8ITPv63JtWlrH3/e9HHDaWcLBZR7q1dbvOyKeCrwHeEFmfrem2gZZO+dlHPhoFW4eB7wwIh7MzN21VDiY2v037DuZ+QDwQER8HjgbMOB0Rzvn5HXA9mwN/Lg1Iv4JOAO4oZ4StYiOv+97uYuqnSUcrgQuqkZXPwu4NzMP1F3oAFn2nETEqcAu4DX+L7Q2y56XzHxiZp6WmacBHwd+13DTde38G3YF8CsRcUxEPBp4JnBzzXUOknbOyR20WtSIiJNprWR9W61VaiEdf9/3bAtOLrKEQ0S8vnr93bSuBnkhcCvwA1rJW13S5jl5K/BY4F1Va8GD6eq8XdXmeVHN2jkvmXlzRHwG+CrwE+A9mbngZbI6em3+rfwR8P6I2EerW+TNmfmdxooeEBHxEeA5wOMi4k7gPwNDsPLve5dqkCRJxenlLipJkqQVMeBIkqTiGHAkSVJxDDiSJKk4BhxJklQcA44kSSqOAUeSJBXHgCNJkopjwJHUsyJibUQciIi3ztn21Ij4YUS8pMnaJPU2ZzKW1NMiYiPwSeDZtFZ53gvckJkuzSJpUQYcST0vIt4BvBj4HPArwDmZ+f1Gi5LU0ww4knpeRDwKuAk4HfjlzLy+4ZIk9TjH4EjqB6cBpwAJPKnZUiT1A1twJPW0iBgCrgNuAa4H3gY8NTPvaLIuSb3NgCOpp0XEduBVwFOBe4G/BoaB52bmT5qsTVLvsotKUs+KiGcDbwIuyszpbP2P7LXAmcCbm6xNUm+zBUeSJBXHFhxJklQcA44kSSqOAUeSJBXHgCNJkopjwJEkScUx4EiSpOIYcCRJUnEMOJIkqTj/Hw3SrBdq2IQSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(x, y)\n",
    "\n",
    "ax.set_title('Data Points to Model')\n",
    "ax.set_xlabel('x', fontsize=14)\n",
    "ax.set_ylabel('y', fontsize=14)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 60)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we wanted to make a best-fit line, what would you guess? Let's create a couple functions to make this easier to make a guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8,
     24
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plotting a guess of a regression line\n",
    "def regression_formula(x, a, b):\n",
    "    return a*x + b\n",
    "\n",
    "def plot_data_and_guess(slope, intercept, ax, x1=x, x2=y, **kwargs):\n",
    "    '''\n",
    "    Plot our data and regression line on the given axis.\n",
    "\n",
    "    Arguments:\n",
    "        slope : float\n",
    "            Value for the slope the regression line.\n",
    "            \n",
    "        intercept : float\n",
    "            Value for the intercept the regression line.\n",
    "        \n",
    "        ax : Axes\n",
    "            Axis to plot data and regression line\n",
    "        \n",
    "        x1 : array-like\n",
    "            Values along the x-axis\n",
    "        \n",
    "        x2 : array-like\n",
    "            Values along the y-axis\n",
    "        \n",
    "    Returns:\n",
    "        fig : Figure\n",
    "\n",
    "        ax : Axes\n",
    "    '''\n",
    "    # Plot data and regression line\n",
    "    ax.scatter(x1, x2)\n",
    "    yhat = regression_formula(x1, slope, intercept)\n",
    "    ax.plot(x1, yhat, 'r-', **kwargs)\n",
    "    \n",
    "    # Embelishments\n",
    "    ax.set_title('Data Points to Model')\n",
    "    ax.set_xlabel('x', fontsize=14)\n",
    "    ax.set_ylabel('y', fontsize=14)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 60)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So what do you think the regression parameters are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Our guess\n",
    "guess = {\n",
    "    'slope': 30,\n",
    "    'intercept': 0\n",
    "}\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_data_and_guess(**guess, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What would be your next guess be? \n",
    "\n",
    "- How can we tell when our guess is \"better\"?\n",
    "- Could we formalize this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One way we can know how well our guess or _model_ did is to compare the predicted values with the actual values. These are the _residuals_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So this would give us the error for each data point:\n",
    "\n",
    "$$ r_i = \\hat{y}_i - y_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculate_residuals(x_values, y_values, slope, intercept):\n",
    "    '''Find the residulas for each data point'''\n",
    "    yhat = intercept + slope*x_values\n",
    "    errors = y_values - yhat\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then we can go further by having just one number to represent how faithful our model was to the actual y-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This leads us to the idea of the **mean squared error** or **MSE**. This is all the residuals squared and then averaged:\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i}^{n} (\\hat{y}_i - y_i)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mse(x_values, y_values, slope, intercept):\n",
    "    \n",
    "    resid_sq = calculate_residuals(x_values, y_values, slope, intercept)**2 \n",
    "\n",
    "    return sum(resid_sq) / len(x_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use our guess from earlier\n",
    "slope = guess.get('slope')\n",
    "intercept = guess.get('intercept')\n",
    "\n",
    "mse(x, y, slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> The function we use to find how bad our model did in prediction is typically called the **loss function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What we found here is great! We can now compare different models with one another.\n",
    "\n",
    "If we made a few different guesses, we could make our predictions and then calculate from the _loss function_ how good or bad our model did! We will want to find the _smallest loss_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now our model changes based on the different model _parameters_ (the coefficients $\\beta_i$ for linear regression). \n",
    "\n",
    "If we imagine all the different ways we can adjust these parameters $\\vec{\\theta}$ and measure how well the model performs with the loss or **cost function** $J(\\vec{\\theta})$, we can plot this as a surface in this multidimensional plane. See the image below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/gradientdescent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note that the terms **loss function** and **cost function** are frequently used interchangeably. Sometimes they are the same function, but sometimes they differ by making changes in the cost to improve _training_ or _learning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try creating the cost function's curve/surface for just one parameter (slope) using our earlier data example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table = np.zeros((20, 2))\n",
    "# Find the MSE for different slope values\n",
    "for idx, val in enumerate(range(40, 60)):\n",
    "    table[idx, 0] = val\n",
    "    table[idx, 1] = mse(x, y, slope=val, intercept=0)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(table[:, 0], table[:, 1], '-')\n",
    "plt.xlabel(\"Slope Values\", fontsize=14)\n",
    "plt.ylabel(\"MSE\", fontsize=14)\n",
    "plt.title(\"MSE with changes to slope\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Based on this graph, what is the optimal slope value?\n",
    "\n",
    "How could we extend this to find the best slope _and_ intercept combination?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Better Way of Guessing: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So this probably all sounds great! We just need to find the minimum of the cost function!\n",
    "\n",
    "But there's some bad news; we don't usually know what the cost function (which can be complicated!) \"looks\" like without trying a whole lot of different parameters $\\vec{\\theta}$. We'd need an _infinite_ number of parameter combinations to know $J(\\vec{\\theta})$ completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So what can we do?\n",
    "\n",
    "Well, we can take one \"guess\" (set of  parameters) and then measure $J(\\vec{\\theta})$. Then we can adjust our guess/parameters in a \"good\" direction, \"down the hill\". This is the basic idea of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Gradient descent** is an optimization procedure that uses the _gradient_ (a generalized notion of a derivative) of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So how do we find this \"better\" guess? Well, we need to find the best direction to move \"downhill\" the fastest. We can do this with a generalization of the derivative called the **gradient**:\n",
    "\n",
    "$$\\begin{align}\\\\\n",
    "    \\large -\\nabla J &= -\\sum_i \\dfrac{\\partial J}{\\partial \\theta_i}\\hat{\\theta_i} \\\\\n",
    "            &= -\\frac{\\partial J}{\\partial \\theta_1}\\hat{\\theta_1} + \\dots +  \\frac{\\partial J}{\\partial \\theta_n}\\hat{\\theta_n}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the multivariate case, the gradient tells us how the function is changing **in each dimension**. A large value of the derivative with respect to a particular variable means that the gradient will have a large component in the corresponding direction. Therefore, **the gradient will point in the direction of steepest increase**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Gradient Descent in Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Make a guess at where the function attains its minimum value\n",
    "- Calculate the gradient/derivative at that point\n",
    "- Use that value to decide how to make your next guess!\n",
    "\n",
    "Repeat until we get the derivative as close as we like to 0.\n",
    "\n",
    "If we want to improve our guess at the minimum of our loss function, we'll move in the **opposite direction** of the gradient away from our last guess. Hence we are using the *gradient* of our loss function to *descend* to the minimum value of the relevant loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Stepping Down a Hill: Step Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So we now have the basic idea of gradient descent of \"going down a hill\" and hopefully it's obvious that the steeper the hill, the more we can adjust our parameters to get to \"bottom\" (optimal parameters) faster.\n",
    "\n",
    "But a big question is how big of a step do we take?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> The amount we adjust our parameter is determined by our **step size**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If our steps are _too big_, we risk skipping over the minimum value (optimal parameters).\n",
    "\n",
    "If our steps are _too small_, it might take us too long to reach the minimum value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![learning_rate](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here's an elegant solution: Make the size of your step **proportional to the value of the derivative at the point where you currently are in parameter space**! If we're very far from the minimum, then our values will be large, and so we therefore ca safely take a large step; if we're close to the minimum, then our values will be small, and so we should therefore take a smaller step.\n",
    "\n",
    "I said the size of the step is proportional to the value of the derivative. The constant of proportionality is often called the **\"learning rate\"**. \n",
    "\n",
    "This page helps to explain the dangers of learning rates that are too large and too small: https://www.jeremyjordan.me/nn-learning-rate/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note there are other optimizations we can do for gradient descent that rely on adjusting our cost function or how we take steps or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The general algorithm looks like this:\n",
    "\n",
    "We'll make a guess, $\\vec{s}$, at where our loss function attains a minimum. If we're not happy with how close the value of the gradient there is to 0, then we'll make a new guess, and the new guess will be constructed as follows:\n",
    "\n",
    "$\\large\\vec{s}_{new} = \\vec{s}_{old} - \\alpha\\nabla f(\\vec{s}_{old})$,\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "In the one-dimensional case, we'll have:\n",
    "\n",
    "$\\large x_{new} = x_{old} - \\alpha\\frac{df}{dx}|_{x_{old}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Gradient Descent Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's go back to our original example and implement gradient descent to find the optimal parameters (slope and intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(x, y)\n",
    "\n",
    "ax.set_title('Data Points to Model')\n",
    "ax.set_xlabel('x', fontsize=14)\n",
    "ax.set_ylabel('y', fontsize=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First we need to find the gradient for the cost function (2-dimensions: $a$ & $b$; slope & intercept):\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial b}\\frac{1}{n}\\Sigma(y_i - (b + ax_i))^2 = -\\frac{2}{n}\\Sigma (y_i-ax_i - b)$$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial a}\\frac{1}{n}\\Sigma(y_i - (b + ax_i))^2 = -\\frac{2}{n}\\Sigma x_i (y_i-ax_i - b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's formalize this into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def partial_deriv(a, b, x_i, y_i, respect_to):\n",
    "    '''\n",
    "    Get the partial derivative for cost function with respect to slope (a) \n",
    "    or intercept (b).\n",
    "    '''\n",
    "    if respect_to == 'b': # intercept\n",
    "        return (y_i - (a * x_i + b))\n",
    "    elif respect_to == 'a': # slope\n",
    "        return (x_i * (y_i - (a * x_i + b)))\n",
    "    else:\n",
    "        print('Choose either respect_to: a or b ')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe: In the code above we've left out both the factors of two and the averages!\n",
    "\n",
    "- We'll take care of the averages below, but this is easily done because **the derivative of a sum is equal to the sum of the derivatives**: $\\frac{d}{dx}[f(x) + g(x)] = \\frac{df}{dx} + \\frac{dg}{dx}$.\n",
    "\n",
    "- The factors of two won't make any difference to our goals. Very often the cost function associated with some modeling task will be something like MSE and so have a squared term, and so then when we differentiate it we'll gain a factor of two. Clearly, minimizing $f(\\beta)$ and minimizing $2f(\\beta)$ will yield the same optimal $\\beta$, and so it's often convenient to leave off the factor of two from the expression of the derivative and so minimize the **half mean squared error** function: $\\frac{1}{2}\\Sigma(y - \\hat{y})^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next let's define the step we take (amount we adjust the parameters by) using the gradient and learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def step_gradient(a, b, x, y, learning_rate):\n",
    "    db = 0\n",
    "    da = 0 \n",
    "    # For each data point, update the derivative for the slope & intercept\n",
    "    N = len(x)\n",
    "    for i in range(N):\n",
    "        \n",
    "        # Partial derivatives of loss/cost function with respect to b & a\n",
    "        # Here's where we're taking our averages. Notice that we're leaving\n",
    "        # off the factors of 2.\n",
    "        db +=  -(1/N) * partial_deriv(a, b, x[i], y[i], respect_to='b')\n",
    "        da +=  -(1/N) * partial_deriv(a, b, x[i], y[i], respect_to='a')\n",
    "        \n",
    "    # Adjust the slope & intercept by the gradient\n",
    "    new_b = b - (learning_rate * db)\n",
    "    new_a = a - (learning_rate * da)\n",
    "    \n",
    "    return (new_a, new_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try it out and keep track of our guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "guesses = []\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Our guess\n",
    "guess = {\n",
    "    'slope': 60,\n",
    "    'intercept': 10\n",
    "}\n",
    "\n",
    "guesses.append(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_data_and_guess(**guess, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "step = step_gradient(guess['slope'], guess['intercept'], x, y, learning_rate=alpha)\n",
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mse(x, y, guess['slope'], guess['intercept'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's update our guess and try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Our guess using the new step\n",
    "new_slope, new_intercept = step\n",
    "guess = {\n",
    "    'slope': new_slope,\n",
    "    'intercept': new_intercept\n",
    "}\n",
    "guesses.append(guess)\n",
    "\n",
    "# Getting adjusted parameters\n",
    "step = step_gradient(guess['slope'], guess['intercept'], x, y, learning_rate=alpha)\n",
    "display(step)\n",
    "display(mse(x, y, guess['slope'], guess['intercept']))\n",
    "\n",
    "# Plotting out our new parameters\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_data_and_guess(**guess, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's repeat this another 200 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    # Our guess using the new step\n",
    "    new_slope, new_intercept = step\n",
    "    guess = {\n",
    "        'slope': new_slope,\n",
    "        'intercept': new_intercept\n",
    "    }\n",
    "    guesses.append(guess)\n",
    "\n",
    "    # Getting adjusted parameters\n",
    "    step = step_gradient(guess['slope'], guess['intercept'], x, y, learning_rate=alpha)\n",
    "    #  Only display every 10\n",
    "    if (i % 10) == 0:\n",
    "        print(f'Step # {i}:')\n",
    "        display(step)\n",
    "        display(mse(x, y, guess['slope'], guess['intercept']))\n",
    "        print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What does our final result look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plotting out our new parameters\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_data_and_guess(**guesses[-1], ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's take a look at the MSE over the guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mses = [\n",
    "    mse(x, y, d['slope'], d['intercept']) for d in guesses\n",
    "]\n",
    "plt.plot(range(len(mses)), mses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This was what we had for 200 iterations. What could we do to improve or speed up this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
